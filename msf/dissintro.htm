<!--This file created 5:29 PM  8/10/97 by Claris Home Page version 2.0-->
<HTML>
<HEAD>
   <TITLE>Multimedia Sensor Fusion for Intelligent Camera
   Control</TITLE>
   <META NAME=GENERATOR CONTENT="Claris Home Page 2.0">
   <X-SAS-WINDOW TOP=52 BOTTOM=451 LEFT=4 RIGHT=651>
</HEAD>
<BODY BGCOLOR="#FFFFFF">

<P><CENTER><B><FONT SIZE="+3" COLOR="#0000AF">Multimedia Sensor
Fusion for Intelligent Camera Control and Human-Computer
Interaction</FONT></B></CENTER></P>

<P><B><FONT SIZE="+2">
<HR>
</FONT></B><FONT SIZE="+1"><A HREF="abstract.htm">Steven Goodridge's
dissertation is available <B>here</B> on the World Wide Web in HTML
format.</A></FONT></P>

<P>This research involves the synergistic use of sound localization
and color vision for tracking human faces and controlling an
automatic camera. This is useful for videoconferencing and
surveillance applications, and offers new capabilities for
human-computer interaction. Inter-aural delay between two microphone
signals allows estimation of the direction to sound, while skin tone
and motion detection allows the visual sensing of faces and body
movement. Information from these sensors is combined ("fused") at the
pixel level in order to detect "noisy face pixels" and track the
position of the person speaking. This information is used to guide a
fuzzy behavior-based camera control system.</P>

<P>The equipment used for this experiment constists of an ordinary
multimedia PC (with a consumer-grade video capture card) attached to
two cameras and two microphones, as shown here:</P>

<BLOCKQUOTE><P><CENTER><FONT SIZE="+1"><IMG SRC="cam.jpg" WIDTH=255
HEIGHT=259 X-SAS-UseImageWidth X-SAS-UseImageHeight
ALIGN=bottom></FONT></CENTER></P></BLOCKQUOTE>

<P>Cross-correlation power of onset signals generated from binaural
sound data is combined with skin-tone information at the pixel level
to detect "noisy skin-colored pixels" using Bayesian classification
techniques:</P>

<P><CENTER><FONT SIZE="+1"><IMG SRC="fuse.jpg" WIDTH=481 HEIGHT=345
X-SAS-UseImageWidth X-SAS-UseImageHeight ALIGN=bottom></FONT>
</CENTER></P>

<P>This information can be tracked for multiple people
simultaneously. The following shows face tracking (red) and pure
sound localization (green) for a typical tracking experiment where
the camera has zoomed in on one person:</P>

<P><CENTER><FONT SIZE="+1"><IMG SRC="face.jpg" WIDTH=136 HEIGHT=108
X-SAS-UseImageWidth X-SAS-UseImageHeight ALIGN=bottom></FONT>
</CENTER></P>

<P>When this data is combined with a sophisticated behavior-based
control system, automatic control of a camera for videoconferencing
can be achieved. The following illustrates how the camera can zoom
out to view multiple people speaking, and zoom in when one person
dominates the conversation for a long enough period of time:</P>

<P><CENTER><FONT SIZE="+1"><IMG SRC="conf.jpg" WIDTH=431 HEIGHT=161
X-SAS-UseImageWidth X-SAS-UseImageHeight ALIGN=bottom></FONT>
</CENTER></P>

<P>Future research will involve the combination of this information
with speech recognition and voice identification, to determine which
person said what word, and interpret this information in the context
of the location and identity of the speaker.</P>

<P><FONT SIZE="+1"><A HREF="abstract.htm">View this dissertation online</A></FONT></P>

<P><FONT SIZE="+1">&nbsp;</FONT></P>

<P><FONT SIZE="+1" COLOR="#FFFFFF">Keywords: Face tracking, sound
localization, face detection, face recognition</FONT></P>

<P>&nbsp;</P>
</BODY>
</HTML>
